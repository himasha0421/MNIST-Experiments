{"cells":[{"metadata":{"id":"5yHlBxX0B4dM"},"cell_type":"markdown","source":"\nFrom Kaggle: \n\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n\n[Read more.](https://www.kaggle.com/c/digit-recognizer)\n\n\n<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"},{"metadata":{"id":"DoOuzSSsB4dZ"},"cell_type":"markdown","source":"### Pytorch Advantages vs Tensorflow\n- Pytorch Enables dynamic computational graphs (which change be changed) while Tensorflow is static. \n- Tensorflow enables easier deployment. "},{"metadata":{},"cell_type":"markdown","source":"# Question 1"},{"metadata":{"id":"IjQEbsJQB4db","trusted":true},"cell_type":"code","source":"#Import Libraries\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":43,"outputs":[]},{"metadata":{"id":"rCwElksrB4dg","trusted":true},"cell_type":"code","source":"#define the training parameters\nbatch_size=32\ntest_batch_size=32\nepochs=30  #The number of Epochs is the number of times you go through the full dataset. \n\nseed=1 #random seed\nlog_interval=10 #log data about statics\ncuda=True #use gpu training\n","execution_count":22,"outputs":[]},{"metadata":{"id":"fAMy35hzB4dl","trusted":true},"cell_type":"code","source":"#load the data\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5,), (0.5,))\n                   ])),\n    batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.5,), (0.5,))\n                   ])),\n    batch_size=test_batch_size, shuffle=True)\n","execution_count":23,"outputs":[]},{"metadata":{"id":"1QUNpCqQB4du","trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    #This defines the structure of the NN.\n    def __init__(self , activation='relu'):\n        super(Net, self).__init__()\n        self.img_size = 28*28\n        self.fc1 = nn.Linear(self.img_size,1024)          # fully connected layer 1\n        self.fc2 = nn.Linear(1024, 1024)                   # fully connected layer 2\n        self.fc3 = nn.Linear(1024,1024 )                     # fully connected layer 3\n        self.fc4 = nn.Linear(1024,1024 )                   # fully connected layer 4\n        self.fc5 = nn.Linear(1024, 1024 )                    # fully connected layer 5\n        self.out_layer = nn.Linear(1024,10)                # output layer\n        #select the activation function\n        if(activation=='relu'):\n            self.activation_fn = nn.ReLU()\n        if(activation=='logistic_sigmoid'):\n            self.activation_fn = nn.LogSigmoid()\n\n    def forward(self, x):\n        #flatten the input vector\n        x = x.view(-1, self.img_size)\n        #Linear Layer 1 /Activation\n        x = self.activation_fn( self.fc1(x) ) \n        #Linear Layer 2 /Activation\n        x = self.activation_fn( self.fc2(x) ) \n        #Linear Layer 3 /Activation\n        x = self.activation_fn( self.fc3(x) ) \n        #Linear Layer 4 /Activation\n        x = self.activation_fn( self.fc4(x) ) \n        #Linear Layer 5 /Activation\n        x = self.activation_fn( self.fc5(x) ) \n        \n        out = self.out_layer(x)\n        #Softmax gets probabilities. \n        return out\n\n#model weight initialization function \ndef init_weights_normal(m):\n    if type(m) == nn.Linear:\n    \n        torch.nn.init.normal_(m.weight , mean=0 , std=0.01)\n        m.bias.data.fill_(0.01)\n\ndef init_weights_xavier(m):\n    if type(m) == nn.Linear:      \n        torch.nn.init.xavier_normal_(m.weight ,gain=0.5)\n        m.bias.data.fill_(0)\ndef init_weights_kaiman(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.kaiming_normal_(m.weight)\n        m.bias.data.fill_(0)","execution_count":24,"outputs":[]},{"metadata":{"id":"SlGbEfqrB4dy","trusted":true},"cell_type":"code","source":"#training the model\ndef train(epoch , model , optimizer):\n    model_test_accuracy = []\n    model.train()\n    for i_epoch in range(epoch):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            if cuda:\n                data, target = data.cuda(), target.cuda()\n            #Variables in Pytorch are differenciable. \n            data, target = Variable(data), Variable(target)\n            #This will zero out the gradients for this batch. \n            optimizer.zero_grad()\n            output = model(data)\n            # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n            loss = criterion(output, target)\n            #dloss/dx for every Variable \n            loss.backward()\n            #to do a one-step update on our parameter.\n            optimizer.step()\n            #Print out the loss periodically. \n            if((i_epoch+1)%30==0):\n                if batch_idx % log_interval == 0:\n                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        i_epoch, batch_idx * len(data), len(train_loader.dataset),\n                        100. * batch_idx / len(train_loader), loss.detach().item()))\n            \n\n        model.eval()\n        test_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for data, target in test_loader:\n                if cuda:\n                    data, target = data.cuda(), target.cuda()\n                data, target = data, target\n                output = model(data)\n                test_loss +=criterion(output, target).detach().item() # sum up batch loss\n                pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n                correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n\n            test_loss /= len(test_loader.dataset)\n            if((i_epoch+1)%30==0):\n                print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n                    test_loss, correct, len(test_loader.dataset),\n                    100. * correct / len(test_loader.dataset)))\n        \n        model.train()\n\n        model_test_accuracy.append(100. * correct / len(test_loader.dataset))\n    return model_test_accuracy","execution_count":25,"outputs":[]},{"metadata":{"id":"8jeiSk8NOxhX"},"cell_type":"markdown","source":"# Model 1 Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"","execution_count":26,"outputs":[]},{"metadata":{"id":"i_tUNVFfdQg1","outputId":"8d3c5a06-c7e7-45f3-cde6-7da52122a10e","trusted":true},"cell_type":"code","source":"# --------------------------> activation =relu |  init_method=kaiman\nmodel = Net(activation='relu')\nmodel.apply(init_weights_kaiman)\nif cuda:\n    model.cuda()\noptimizer_adam = optim.Adam(model.parameters() , lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n#Adam Training\n\nmodel5_ADAM_accuracy  = train(epochs ,model ,  optimizer_adam)","execution_count":27,"outputs":[{"output_type":"stream","text":"Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000068\nTrain Epoch: 29 [320/60000 (1%)]\tLoss: 0.000011\nTrain Epoch: 29 [640/60000 (1%)]\tLoss: 0.000071\nTrain Epoch: 29 [960/60000 (2%)]\tLoss: 0.000001\nTrain Epoch: 29 [1280/60000 (2%)]\tLoss: 0.000438\nTrain Epoch: 29 [1600/60000 (3%)]\tLoss: 0.000930\nTrain Epoch: 29 [1920/60000 (3%)]\tLoss: 0.000012\nTrain Epoch: 29 [2240/60000 (4%)]\tLoss: 0.048179\nTrain Epoch: 29 [2560/60000 (4%)]\tLoss: 0.000197\nTrain Epoch: 29 [2880/60000 (5%)]\tLoss: 0.000110\nTrain Epoch: 29 [3200/60000 (5%)]\tLoss: 0.000029\nTrain Epoch: 29 [3520/60000 (6%)]\tLoss: 0.015260\nTrain Epoch: 29 [3840/60000 (6%)]\tLoss: 0.000061\nTrain Epoch: 29 [4160/60000 (7%)]\tLoss: 0.000259\nTrain Epoch: 29 [4480/60000 (7%)]\tLoss: 0.000122\nTrain Epoch: 29 [4800/60000 (8%)]\tLoss: 0.000048\nTrain Epoch: 29 [5120/60000 (9%)]\tLoss: 0.000108\nTrain Epoch: 29 [5440/60000 (9%)]\tLoss: 0.000090\nTrain Epoch: 29 [5760/60000 (10%)]\tLoss: 0.000000\nTrain Epoch: 29 [6080/60000 (10%)]\tLoss: 0.000709\nTrain Epoch: 29 [6400/60000 (11%)]\tLoss: 0.000230\nTrain Epoch: 29 [6720/60000 (11%)]\tLoss: 0.000002\nTrain Epoch: 29 [7040/60000 (12%)]\tLoss: 0.000511\nTrain Epoch: 29 [7360/60000 (12%)]\tLoss: 0.000066\nTrain Epoch: 29 [7680/60000 (13%)]\tLoss: 0.000018\nTrain Epoch: 29 [8000/60000 (13%)]\tLoss: 0.000366\nTrain Epoch: 29 [8320/60000 (14%)]\tLoss: 0.000124\nTrain Epoch: 29 [8640/60000 (14%)]\tLoss: 0.000166\nTrain Epoch: 29 [8960/60000 (15%)]\tLoss: 0.000505\nTrain Epoch: 29 [9280/60000 (15%)]\tLoss: 0.000001\nTrain Epoch: 29 [9600/60000 (16%)]\tLoss: 0.013676\nTrain Epoch: 29 [9920/60000 (17%)]\tLoss: 0.000242\nTrain Epoch: 29 [10240/60000 (17%)]\tLoss: 0.000006\nTrain Epoch: 29 [10560/60000 (18%)]\tLoss: 0.000781\nTrain Epoch: 29 [10880/60000 (18%)]\tLoss: 0.000015\nTrain Epoch: 29 [11200/60000 (19%)]\tLoss: 0.000002\nTrain Epoch: 29 [11520/60000 (19%)]\tLoss: 0.000075\nTrain Epoch: 29 [11840/60000 (20%)]\tLoss: 0.054100\nTrain Epoch: 29 [12160/60000 (20%)]\tLoss: 0.000117\nTrain Epoch: 29 [12480/60000 (21%)]\tLoss: 0.006252\nTrain Epoch: 29 [12800/60000 (21%)]\tLoss: 0.000617\nTrain Epoch: 29 [13120/60000 (22%)]\tLoss: 0.001775\nTrain Epoch: 29 [13440/60000 (22%)]\tLoss: 0.000025\nTrain Epoch: 29 [13760/60000 (23%)]\tLoss: 0.000121\nTrain Epoch: 29 [14080/60000 (23%)]\tLoss: 0.008184\nTrain Epoch: 29 [14400/60000 (24%)]\tLoss: 0.000785\nTrain Epoch: 29 [14720/60000 (25%)]\tLoss: 0.000229\nTrain Epoch: 29 [15040/60000 (25%)]\tLoss: 0.000013\nTrain Epoch: 29 [15360/60000 (26%)]\tLoss: 0.000041\nTrain Epoch: 29 [15680/60000 (26%)]\tLoss: 0.000009\nTrain Epoch: 29 [16000/60000 (27%)]\tLoss: 0.000002\nTrain Epoch: 29 [16320/60000 (27%)]\tLoss: 0.001575\nTrain Epoch: 29 [16640/60000 (28%)]\tLoss: 0.124850\nTrain Epoch: 29 [16960/60000 (28%)]\tLoss: 0.000728\nTrain Epoch: 29 [17280/60000 (29%)]\tLoss: 0.001022\nTrain Epoch: 29 [17600/60000 (29%)]\tLoss: 0.131242\nTrain Epoch: 29 [17920/60000 (30%)]\tLoss: 0.000555\nTrain Epoch: 29 [18240/60000 (30%)]\tLoss: 0.000380\nTrain Epoch: 29 [18560/60000 (31%)]\tLoss: 0.040468\nTrain Epoch: 29 [18880/60000 (31%)]\tLoss: 0.001248\nTrain Epoch: 29 [19200/60000 (32%)]\tLoss: 0.009970\nTrain Epoch: 29 [19520/60000 (33%)]\tLoss: 0.000005\nTrain Epoch: 29 [19840/60000 (33%)]\tLoss: 0.021876\nTrain Epoch: 29 [20160/60000 (34%)]\tLoss: 0.000787\nTrain Epoch: 29 [20480/60000 (34%)]\tLoss: 0.000003\nTrain Epoch: 29 [20800/60000 (35%)]\tLoss: 0.050799\nTrain Epoch: 29 [21120/60000 (35%)]\tLoss: 0.000004\nTrain Epoch: 29 [21440/60000 (36%)]\tLoss: 0.187889\nTrain Epoch: 29 [21760/60000 (36%)]\tLoss: 0.194860\nTrain Epoch: 29 [22080/60000 (37%)]\tLoss: 0.000000\nTrain Epoch: 29 [22400/60000 (37%)]\tLoss: 0.029486\nTrain Epoch: 29 [22720/60000 (38%)]\tLoss: 0.000044\nTrain Epoch: 29 [23040/60000 (38%)]\tLoss: 0.000408\nTrain Epoch: 29 [23360/60000 (39%)]\tLoss: 0.000031\nTrain Epoch: 29 [23680/60000 (39%)]\tLoss: 0.000363\nTrain Epoch: 29 [24000/60000 (40%)]\tLoss: 0.000293\nTrain Epoch: 29 [24320/60000 (41%)]\tLoss: 0.000161\nTrain Epoch: 29 [24640/60000 (41%)]\tLoss: 0.000010\nTrain Epoch: 29 [24960/60000 (42%)]\tLoss: 0.000009\nTrain Epoch: 29 [25280/60000 (42%)]\tLoss: 0.000116\nTrain Epoch: 29 [25600/60000 (43%)]\tLoss: 0.009789\nTrain Epoch: 29 [25920/60000 (43%)]\tLoss: 0.000042\nTrain Epoch: 29 [26240/60000 (44%)]\tLoss: 0.000150\nTrain Epoch: 29 [26560/60000 (44%)]\tLoss: 0.000001\nTrain Epoch: 29 [26880/60000 (45%)]\tLoss: 0.000058\nTrain Epoch: 29 [27200/60000 (45%)]\tLoss: 0.001526\nTrain Epoch: 29 [27520/60000 (46%)]\tLoss: 0.000008\nTrain Epoch: 29 [27840/60000 (46%)]\tLoss: 0.000011\nTrain Epoch: 29 [28160/60000 (47%)]\tLoss: 0.000067\nTrain Epoch: 29 [28480/60000 (47%)]\tLoss: 0.000046\nTrain Epoch: 29 [28800/60000 (48%)]\tLoss: 0.002516\nTrain Epoch: 29 [29120/60000 (49%)]\tLoss: 0.002556\nTrain Epoch: 29 [29440/60000 (49%)]\tLoss: 0.000035\nTrain Epoch: 29 [29760/60000 (50%)]\tLoss: 0.009263\nTrain Epoch: 29 [30080/60000 (50%)]\tLoss: 0.000006\nTrain Epoch: 29 [30400/60000 (51%)]\tLoss: 0.000004\nTrain Epoch: 29 [30720/60000 (51%)]\tLoss: 0.000247\nTrain Epoch: 29 [31040/60000 (52%)]\tLoss: 0.022033\nTrain Epoch: 29 [31360/60000 (52%)]\tLoss: 0.000218\nTrain Epoch: 29 [31680/60000 (53%)]\tLoss: 0.004698\nTrain Epoch: 29 [32000/60000 (53%)]\tLoss: 0.002504\nTrain Epoch: 29 [32320/60000 (54%)]\tLoss: 0.000079\nTrain Epoch: 29 [32640/60000 (54%)]\tLoss: 0.008726\nTrain Epoch: 29 [32960/60000 (55%)]\tLoss: 0.000073\nTrain Epoch: 29 [33280/60000 (55%)]\tLoss: 0.001729\nTrain Epoch: 29 [33600/60000 (56%)]\tLoss: 0.026873\nTrain Epoch: 29 [33920/60000 (57%)]\tLoss: 0.000008\nTrain Epoch: 29 [34240/60000 (57%)]\tLoss: 0.000270\nTrain Epoch: 29 [34560/60000 (58%)]\tLoss: 0.000193\nTrain Epoch: 29 [34880/60000 (58%)]\tLoss: 0.000315\nTrain Epoch: 29 [35200/60000 (59%)]\tLoss: 0.003217\nTrain Epoch: 29 [35520/60000 (59%)]\tLoss: 0.000075\nTrain Epoch: 29 [35840/60000 (60%)]\tLoss: 0.001095\nTrain Epoch: 29 [36160/60000 (60%)]\tLoss: 0.000000\nTrain Epoch: 29 [36480/60000 (61%)]\tLoss: 0.000004\nTrain Epoch: 29 [36800/60000 (61%)]\tLoss: 0.000180\nTrain Epoch: 29 [37120/60000 (62%)]\tLoss: 0.000489\nTrain Epoch: 29 [37440/60000 (62%)]\tLoss: 0.000053\nTrain Epoch: 29 [37760/60000 (63%)]\tLoss: 0.001977\nTrain Epoch: 29 [38080/60000 (63%)]\tLoss: 0.000028\nTrain Epoch: 29 [38400/60000 (64%)]\tLoss: 0.001905\nTrain Epoch: 29 [38720/60000 (65%)]\tLoss: 0.000280\nTrain Epoch: 29 [39040/60000 (65%)]\tLoss: 0.000037\nTrain Epoch: 29 [39360/60000 (66%)]\tLoss: 0.000714\nTrain Epoch: 29 [39680/60000 (66%)]\tLoss: 0.004114\nTrain Epoch: 29 [40000/60000 (67%)]\tLoss: 0.000060\nTrain Epoch: 29 [40320/60000 (67%)]\tLoss: 0.058791\nTrain Epoch: 29 [40640/60000 (68%)]\tLoss: 0.011665\nTrain Epoch: 29 [40960/60000 (68%)]\tLoss: 0.005105\nTrain Epoch: 29 [41280/60000 (69%)]\tLoss: 0.019325\nTrain Epoch: 29 [41600/60000 (69%)]\tLoss: 0.000954\nTrain Epoch: 29 [41920/60000 (70%)]\tLoss: 0.003041\nTrain Epoch: 29 [42240/60000 (70%)]\tLoss: 0.000001\nTrain Epoch: 29 [42560/60000 (71%)]\tLoss: 0.000169\nTrain Epoch: 29 [42880/60000 (71%)]\tLoss: 0.000045\nTrain Epoch: 29 [43200/60000 (72%)]\tLoss: 0.000015\nTrain Epoch: 29 [43520/60000 (73%)]\tLoss: 0.117855\nTrain Epoch: 29 [43840/60000 (73%)]\tLoss: 0.000884\nTrain Epoch: 29 [44160/60000 (74%)]\tLoss: 0.016247\nTrain Epoch: 29 [44480/60000 (74%)]\tLoss: 0.000115\nTrain Epoch: 29 [44800/60000 (75%)]\tLoss: 0.000029\nTrain Epoch: 29 [45120/60000 (75%)]\tLoss: 0.006688\nTrain Epoch: 29 [45440/60000 (76%)]\tLoss: 0.000288\nTrain Epoch: 29 [45760/60000 (76%)]\tLoss: 0.003592\nTrain Epoch: 29 [46080/60000 (77%)]\tLoss: 0.045907\nTrain Epoch: 29 [46400/60000 (77%)]\tLoss: 0.006466\nTrain Epoch: 29 [46720/60000 (78%)]\tLoss: 0.016155\nTrain Epoch: 29 [47040/60000 (78%)]\tLoss: 0.000068\nTrain Epoch: 29 [47360/60000 (79%)]\tLoss: 0.000124\nTrain Epoch: 29 [47680/60000 (79%)]\tLoss: 0.016522\nTrain Epoch: 29 [48000/60000 (80%)]\tLoss: 0.018104\nTrain Epoch: 29 [48320/60000 (81%)]\tLoss: 0.000039\nTrain Epoch: 29 [48640/60000 (81%)]\tLoss: 0.000014\nTrain Epoch: 29 [48960/60000 (82%)]\tLoss: 0.101419\nTrain Epoch: 29 [49280/60000 (82%)]\tLoss: 0.000056\nTrain Epoch: 29 [49600/60000 (83%)]\tLoss: 0.000778\nTrain Epoch: 29 [49920/60000 (83%)]\tLoss: 0.000047\nTrain Epoch: 29 [50240/60000 (84%)]\tLoss: 0.001490\nTrain Epoch: 29 [50560/60000 (84%)]\tLoss: 0.003369\nTrain Epoch: 29 [50880/60000 (85%)]\tLoss: 0.000219\nTrain Epoch: 29 [51200/60000 (85%)]\tLoss: 0.004280\nTrain Epoch: 29 [51520/60000 (86%)]\tLoss: 0.000145\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 29 [51840/60000 (86%)]\tLoss: 0.001015\nTrain Epoch: 29 [52160/60000 (87%)]\tLoss: 0.000062\nTrain Epoch: 29 [52480/60000 (87%)]\tLoss: 0.019131\nTrain Epoch: 29 [52800/60000 (88%)]\tLoss: 0.001034\nTrain Epoch: 29 [53120/60000 (89%)]\tLoss: 0.000005\nTrain Epoch: 29 [53440/60000 (89%)]\tLoss: 0.000261\nTrain Epoch: 29 [53760/60000 (90%)]\tLoss: 0.000196\nTrain Epoch: 29 [54080/60000 (90%)]\tLoss: 0.000698\nTrain Epoch: 29 [54400/60000 (91%)]\tLoss: 0.000031\nTrain Epoch: 29 [54720/60000 (91%)]\tLoss: 0.155675\nTrain Epoch: 29 [55040/60000 (92%)]\tLoss: 0.000348\nTrain Epoch: 29 [55360/60000 (92%)]\tLoss: 0.000041\nTrain Epoch: 29 [55680/60000 (93%)]\tLoss: 0.282977\nTrain Epoch: 29 [56000/60000 (93%)]\tLoss: 0.190254\nTrain Epoch: 29 [56320/60000 (94%)]\tLoss: 0.000033\nTrain Epoch: 29 [56640/60000 (94%)]\tLoss: 0.000066\nTrain Epoch: 29 [56960/60000 (95%)]\tLoss: 0.014275\nTrain Epoch: 29 [57280/60000 (95%)]\tLoss: 0.074085\nTrain Epoch: 29 [57600/60000 (96%)]\tLoss: 0.000045\nTrain Epoch: 29 [57920/60000 (97%)]\tLoss: 0.002200\nTrain Epoch: 29 [58240/60000 (97%)]\tLoss: 0.000312\nTrain Epoch: 29 [58560/60000 (98%)]\tLoss: 0.003034\nTrain Epoch: 29 [58880/60000 (98%)]\tLoss: 0.089391\nTrain Epoch: 29 [59200/60000 (99%)]\tLoss: 0.013091\nTrain Epoch: 29 [59520/60000 (99%)]\tLoss: 0.000516\nTrain Epoch: 29 [59840/60000 (100%)]\tLoss: 0.000429\n\nTest set: Average loss: 0.0029, Accuracy: 9807/10000 (98%)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nD_list = [10, 20, 50, 100, 200 , \"all\"]\nlayers_list = np.copy(list(model.children()))\n\nfinal_svd_output = []\n\nfor i_d in D_list :\n\n    svd_info = []\n    for i_layer in layers_list[:-2] :\n        if type(i_layer) == nn.Linear:      \n            u, s, v = torch.svd(i_layer.weight)\n            svd_info.append([u,s,v])\n            \n    model_svd = Net(activation='relu')\n    model_svd.apply(init_weights_kaiman)\n    if cuda:\n        model_svd.cuda()\n            \n    svd_layers = list(model_svd.children()).copy()\n    \n    for i , i_layer in enumerate(svd_layers[:-1] ):\n        if type(i_layer) == nn.Linear:   \n            if(i!=5):\n\n                u, s, v = svd_info[i]\n\n                if(i_d !=\"all\"):\n                    s = s[:i_d]\n                    u = u[:,:i_d]\n                    v = v[:,:i_d]\n                \n                i_layer.weight.data = torch.mm(torch.mm(u, torch.diag(s)), v.t())\n\n            if(i==5):\n                i_layer.weight.data = layers_list[i].weight.data\n                i_layer.bias.data = layers_list[i].bias.data\n            \n    final_svd_output.append(np.copy(svd_layers))","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVD_Net(nn.Module):\n    #This defines the structure of the NN.\n    def __init__(self , layer_list ,  activation='relu'):\n        super(SVD_Net, self).__init__()\n        self.img_size = 28*28\n        self.fc1 = layer_list[0]          # fully connected layer 1\n        self.fc2 = layer_list[1]          # fully connected layer 2 \n        self.fc3 = layer_list[2]          # fully connected layer 3\n        self.fc4 = layer_list[3]          # fully connected layer 4\n        self.fc5 = layer_list[4]          # fully connected layer 5\n        self.out_layer = layer_list[5]    # output layer\n        #select the activation function\n        if(activation=='relu'):\n            self.activation_fn = nn.ReLU()\n        if(activation=='logistic_sigmoid'):\n            self.activation_fn = nn.LogSigmoid()\n\n    def forward(self, x):\n        #flatten the input vector\n        x = x.view(-1, self.img_size)\n        #Linear Layer 1 /Activation\n        x = self.activation_fn( self.fc1(x) ) \n        #Linear Layer 2 /Activation\n        x = self.activation_fn( self.fc2(x) ) \n        #Linear Layer 3 /Activation\n        x = self.activation_fn( self.fc3(x) ) \n        #Linear Layer 4 /Activation\n        x = self.activation_fn( self.fc4(x) ) \n        #Linear Layer 5 /Activation\n        x = self.activation_fn( self.fc5(x) ) \n        \n        out = self.out_layer(x)\n        #Softmax gets probabilities. \n        return out","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_svd(my_model):\n    my_model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            if cuda:\n                data, target = data.cuda(), target.cuda()\n            data, target = data, target\n            output = my_model(data)\n            test_loss +=criterion(output, target).detach().item() # sum up batch loss\n            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n\n            test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n                test_loss, correct, len(test_loader.dataset),\n                100. * correct / len(test_loader.dataset)))\n    return 100. * correct / len(test_loader.dataset)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_accuracy =[]\nfor i_svd_layer in final_svd_output:\n\n    my_model = SVD_Net(i_svd_layer)\n    i_acc = test_svd(my_model)\n    test_accuracy.append(i_acc)","execution_count":31,"outputs":[{"output_type":"stream","text":"\nTest set: Average loss: 0.0001, Accuracy: 4662/10000 (47%)\n\n\nTest set: Average loss: 0.0002, Accuracy: 8861/10000 (89%)\n\n\nTest set: Average loss: 0.0000, Accuracy: 9678/10000 (97%)\n\n\nTest set: Average loss: 0.0000, Accuracy: 9736/10000 (97%)\n\n\nTest set: Average loss: 0.0000, Accuracy: 9769/10000 (98%)\n\n\nTest set: Average loss: 0.0000, Accuracy: 9805/10000 (98%)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(15,8))\n\nplt.plot(test_accuracy, \"-^b\", markersize= 10, label=\"Train Loss\")\nplt.xticks(np.arange(6), [\"D 10\", \"D_20\", \"D_50\", \"D_100\", \"D_200\" , \"D_full\"] ) \nplt.legend(loc=\"upper left\")\nplt.ylim(-0.5, 100.0)","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"(-0.5, 100.0)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# Question 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SVD_Net(nn.Module):\n    #This defines the structure of the NN.\n    def __init__(self , layer_list ,  activation='relu'):\n        super(SVD_Net, self).__init__()\n        self.img_size = 28*28\n        self.fc1 = layer_list[0]          # fully connected layer 1\n        self.fc2 = layer_list[1]          # fully connected layer 2 \n        self.fc3 = layer_list[2]          # fully connected layer 3\n        self.fc4 = layer_list[3]          # fully connected layer 4\n        self.fc5 = layer_list[4]          # fully connected layer 5\n        self.out_layer = layer_list[5]    # output layer\n        #select the activation function\n        if(activation=='relu'):\n            self.activation_fn = nn.ReLU()\n        if(activation=='logistic_sigmoid'):\n            self.activation_fn = nn.LogSigmoid()\n\n    def forward(self, x):\n        #flatten the input vector\n        x = x.view(-1, self.img_size)\n        #Linear Layer 1 /Activation\n        x = self.activation_fn( self.fc1(x) ) \n        #Linear Layer 2 /Activation\n        x = self.activation_fn( self.fc2(x) ) \n        #Linear Layer 3 /Activation\n        x = self.activation_fn( self.fc3(x) ) \n        #Linear Layer 4 /Activation\n        x = self.activation_fn( self.fc4(x) ) \n        #Linear Layer 5 /Activation\n        x = self.activation_fn( self.fc5(x) ) \n        \n        out = self.out_layer(x)\n        #Softmax gets probabilities. \n        return out","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_model = SVD_Net(final_svd_output[1])\ni_acc = test_svd(svd_model)","execution_count":34,"outputs":[{"output_type":"stream","text":"\nTest set: Average loss: 0.0001, Accuracy: 8861/10000 (89%)\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# FineTune the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# --------------------------> activation =relu |  init_method=kaiman\nif cuda:\n    svd_model.cuda()\noptimizer_adam = optim.Adam(svd_model.parameters() , lr=0.00001)\ncriterion = nn.CrossEntropyLoss()\n#Adam Training\n\nmodel5_ADAM_accuracy  = train(epochs , svd_model ,  optimizer_adam)","execution_count":35,"outputs":[{"output_type":"stream","text":"Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000002\nTrain Epoch: 29 [320/60000 (1%)]\tLoss: 0.000000\nTrain Epoch: 29 [640/60000 (1%)]\tLoss: 0.000000\nTrain Epoch: 29 [960/60000 (2%)]\tLoss: 0.000615\nTrain Epoch: 29 [1280/60000 (2%)]\tLoss: 0.001153\nTrain Epoch: 29 [1600/60000 (3%)]\tLoss: 0.000078\nTrain Epoch: 29 [1920/60000 (3%)]\tLoss: 0.000000\nTrain Epoch: 29 [2240/60000 (4%)]\tLoss: 0.000009\nTrain Epoch: 29 [2560/60000 (4%)]\tLoss: 0.000000\nTrain Epoch: 29 [2880/60000 (5%)]\tLoss: 0.000241\nTrain Epoch: 29 [3200/60000 (5%)]\tLoss: 0.000002\nTrain Epoch: 29 [3520/60000 (6%)]\tLoss: 0.000001\nTrain Epoch: 29 [3840/60000 (6%)]\tLoss: 0.000009\nTrain Epoch: 29 [4160/60000 (7%)]\tLoss: 0.000000\nTrain Epoch: 29 [4480/60000 (7%)]\tLoss: 0.000002\nTrain Epoch: 29 [4800/60000 (8%)]\tLoss: 0.000044\nTrain Epoch: 29 [5120/60000 (9%)]\tLoss: 0.000003\nTrain Epoch: 29 [5440/60000 (9%)]\tLoss: 0.000002\nTrain Epoch: 29 [5760/60000 (10%)]\tLoss: 0.000000\nTrain Epoch: 29 [6080/60000 (10%)]\tLoss: 0.000074\nTrain Epoch: 29 [6400/60000 (11%)]\tLoss: 0.000025\nTrain Epoch: 29 [6720/60000 (11%)]\tLoss: 0.000003\nTrain Epoch: 29 [7040/60000 (12%)]\tLoss: 0.000000\nTrain Epoch: 29 [7360/60000 (12%)]\tLoss: 0.000018\nTrain Epoch: 29 [7680/60000 (13%)]\tLoss: 0.000238\nTrain Epoch: 29 [8000/60000 (13%)]\tLoss: 0.000001\nTrain Epoch: 29 [8320/60000 (14%)]\tLoss: 0.000018\nTrain Epoch: 29 [8640/60000 (14%)]\tLoss: 0.000000\nTrain Epoch: 29 [8960/60000 (15%)]\tLoss: 0.000002\nTrain Epoch: 29 [9280/60000 (15%)]\tLoss: 0.000003\nTrain Epoch: 29 [9600/60000 (16%)]\tLoss: 0.000011\nTrain Epoch: 29 [9920/60000 (17%)]\tLoss: 0.000264\nTrain Epoch: 29 [10240/60000 (17%)]\tLoss: 0.000098\nTrain Epoch: 29 [10560/60000 (18%)]\tLoss: 0.000000\nTrain Epoch: 29 [10880/60000 (18%)]\tLoss: 0.000003\nTrain Epoch: 29 [11200/60000 (19%)]\tLoss: 0.000000\nTrain Epoch: 29 [11520/60000 (19%)]\tLoss: 0.000001\nTrain Epoch: 29 [11840/60000 (20%)]\tLoss: 0.000162\nTrain Epoch: 29 [12160/60000 (20%)]\tLoss: 0.000012\nTrain Epoch: 29 [12480/60000 (21%)]\tLoss: 0.000036\nTrain Epoch: 29 [12800/60000 (21%)]\tLoss: 0.000072\nTrain Epoch: 29 [13120/60000 (22%)]\tLoss: 0.000000\nTrain Epoch: 29 [13440/60000 (22%)]\tLoss: 0.000004\nTrain Epoch: 29 [13760/60000 (23%)]\tLoss: 0.000003\nTrain Epoch: 29 [14080/60000 (23%)]\tLoss: 0.000009\nTrain Epoch: 29 [14400/60000 (24%)]\tLoss: 0.000004\nTrain Epoch: 29 [14720/60000 (25%)]\tLoss: 0.000001\nTrain Epoch: 29 [15040/60000 (25%)]\tLoss: 0.000005\nTrain Epoch: 29 [15360/60000 (26%)]\tLoss: 0.000050\nTrain Epoch: 29 [15680/60000 (26%)]\tLoss: 0.000000\nTrain Epoch: 29 [16000/60000 (27%)]\tLoss: 0.000000\nTrain Epoch: 29 [16320/60000 (27%)]\tLoss: 0.000354\nTrain Epoch: 29 [16640/60000 (28%)]\tLoss: 0.001317\nTrain Epoch: 29 [16960/60000 (28%)]\tLoss: 0.000005\nTrain Epoch: 29 [17280/60000 (29%)]\tLoss: 0.000000\nTrain Epoch: 29 [17600/60000 (29%)]\tLoss: 0.000000\nTrain Epoch: 29 [17920/60000 (30%)]\tLoss: 0.000074\nTrain Epoch: 29 [18240/60000 (30%)]\tLoss: 0.000108\nTrain Epoch: 29 [18560/60000 (31%)]\tLoss: 0.000004\nTrain Epoch: 29 [18880/60000 (31%)]\tLoss: 0.000031\nTrain Epoch: 29 [19200/60000 (32%)]\tLoss: 0.000440\nTrain Epoch: 29 [19520/60000 (33%)]\tLoss: 0.000001\nTrain Epoch: 29 [19840/60000 (33%)]\tLoss: 0.000000\nTrain Epoch: 29 [20160/60000 (34%)]\tLoss: 0.000010\nTrain Epoch: 29 [20480/60000 (34%)]\tLoss: 0.000001\nTrain Epoch: 29 [20800/60000 (35%)]\tLoss: 0.000000\nTrain Epoch: 29 [21120/60000 (35%)]\tLoss: 0.000011\nTrain Epoch: 29 [21440/60000 (36%)]\tLoss: 0.000000\nTrain Epoch: 29 [21760/60000 (36%)]\tLoss: 0.000001\nTrain Epoch: 29 [22080/60000 (37%)]\tLoss: 0.000300\nTrain Epoch: 29 [22400/60000 (37%)]\tLoss: 0.000168\nTrain Epoch: 29 [22720/60000 (38%)]\tLoss: 0.000005\nTrain Epoch: 29 [23040/60000 (38%)]\tLoss: 0.000000\nTrain Epoch: 29 [23360/60000 (39%)]\tLoss: 0.000000\nTrain Epoch: 29 [23680/60000 (39%)]\tLoss: 0.000002\nTrain Epoch: 29 [24000/60000 (40%)]\tLoss: 0.000000\nTrain Epoch: 29 [24320/60000 (41%)]\tLoss: 0.000000\nTrain Epoch: 29 [24640/60000 (41%)]\tLoss: 0.000098\nTrain Epoch: 29 [24960/60000 (42%)]\tLoss: 0.000000\nTrain Epoch: 29 [25280/60000 (42%)]\tLoss: 0.000299\nTrain Epoch: 29 [25600/60000 (43%)]\tLoss: 0.000076\nTrain Epoch: 29 [25920/60000 (43%)]\tLoss: 0.000050\nTrain Epoch: 29 [26240/60000 (44%)]\tLoss: 0.000041\nTrain Epoch: 29 [26560/60000 (44%)]\tLoss: 0.000264\nTrain Epoch: 29 [26880/60000 (45%)]\tLoss: 0.000009\nTrain Epoch: 29 [27200/60000 (45%)]\tLoss: 0.000008\nTrain Epoch: 29 [27520/60000 (46%)]\tLoss: 0.000110\nTrain Epoch: 29 [27840/60000 (46%)]\tLoss: 0.000000\nTrain Epoch: 29 [28160/60000 (47%)]\tLoss: 0.000000\nTrain Epoch: 29 [28480/60000 (47%)]\tLoss: 0.000005\nTrain Epoch: 29 [28800/60000 (48%)]\tLoss: 0.000006\nTrain Epoch: 29 [29120/60000 (49%)]\tLoss: 0.000000\nTrain Epoch: 29 [29440/60000 (49%)]\tLoss: 0.000017\nTrain Epoch: 29 [29760/60000 (50%)]\tLoss: 0.000003\nTrain Epoch: 29 [30080/60000 (50%)]\tLoss: 0.000001\nTrain Epoch: 29 [30400/60000 (51%)]\tLoss: 0.000001\nTrain Epoch: 29 [30720/60000 (51%)]\tLoss: 0.000000\nTrain Epoch: 29 [31040/60000 (52%)]\tLoss: 0.000069\nTrain Epoch: 29 [31360/60000 (52%)]\tLoss: 0.000002\nTrain Epoch: 29 [31680/60000 (53%)]\tLoss: 0.005132\nTrain Epoch: 29 [32000/60000 (53%)]\tLoss: 0.000042\nTrain Epoch: 29 [32320/60000 (54%)]\tLoss: 0.000013\nTrain Epoch: 29 [32640/60000 (54%)]\tLoss: 0.000000\nTrain Epoch: 29 [32960/60000 (55%)]\tLoss: 0.000000\nTrain Epoch: 29 [33280/60000 (55%)]\tLoss: 0.000000\nTrain Epoch: 29 [33600/60000 (56%)]\tLoss: 0.000004\nTrain Epoch: 29 [33920/60000 (57%)]\tLoss: 0.000000\nTrain Epoch: 29 [34240/60000 (57%)]\tLoss: 0.000000\nTrain Epoch: 29 [34560/60000 (58%)]\tLoss: 0.000000\nTrain Epoch: 29 [34880/60000 (58%)]\tLoss: 0.000001\nTrain Epoch: 29 [35200/60000 (59%)]\tLoss: 0.000009\nTrain Epoch: 29 [35520/60000 (59%)]\tLoss: 0.000000\nTrain Epoch: 29 [35840/60000 (60%)]\tLoss: 0.000029\nTrain Epoch: 29 [36160/60000 (60%)]\tLoss: 0.000009\nTrain Epoch: 29 [36480/60000 (61%)]\tLoss: 0.000002\nTrain Epoch: 29 [36800/60000 (61%)]\tLoss: 0.000000\nTrain Epoch: 29 [37120/60000 (62%)]\tLoss: 0.000000\nTrain Epoch: 29 [37440/60000 (62%)]\tLoss: 0.000016\nTrain Epoch: 29 [37760/60000 (63%)]\tLoss: 0.000005\nTrain Epoch: 29 [38080/60000 (63%)]\tLoss: 0.000000\nTrain Epoch: 29 [38400/60000 (64%)]\tLoss: 0.000046\nTrain Epoch: 29 [38720/60000 (65%)]\tLoss: 0.000001\nTrain Epoch: 29 [39040/60000 (65%)]\tLoss: 0.000000\nTrain Epoch: 29 [39360/60000 (66%)]\tLoss: 0.000009\nTrain Epoch: 29 [39680/60000 (66%)]\tLoss: 0.000050\nTrain Epoch: 29 [40000/60000 (67%)]\tLoss: 0.000000\nTrain Epoch: 29 [40320/60000 (67%)]\tLoss: 0.000007\nTrain Epoch: 29 [40640/60000 (68%)]\tLoss: 0.000118\nTrain Epoch: 29 [40960/60000 (68%)]\tLoss: 0.000026\nTrain Epoch: 29 [41280/60000 (69%)]\tLoss: 0.000164\nTrain Epoch: 29 [41600/60000 (69%)]\tLoss: 0.000001\nTrain Epoch: 29 [41920/60000 (70%)]\tLoss: 0.000000\nTrain Epoch: 29 [42240/60000 (70%)]\tLoss: 0.000004\nTrain Epoch: 29 [42560/60000 (71%)]\tLoss: 0.000000\nTrain Epoch: 29 [42880/60000 (71%)]\tLoss: 0.000001\nTrain Epoch: 29 [43200/60000 (72%)]\tLoss: 0.000000\nTrain Epoch: 29 [43520/60000 (73%)]\tLoss: 0.000000\nTrain Epoch: 29 [43840/60000 (73%)]\tLoss: 0.000001\nTrain Epoch: 29 [44160/60000 (74%)]\tLoss: 0.022171\nTrain Epoch: 29 [44480/60000 (74%)]\tLoss: 0.000000\nTrain Epoch: 29 [44800/60000 (75%)]\tLoss: 0.000000\nTrain Epoch: 29 [45120/60000 (75%)]\tLoss: 0.000007\nTrain Epoch: 29 [45440/60000 (76%)]\tLoss: 0.000022\nTrain Epoch: 29 [45760/60000 (76%)]\tLoss: 0.000035\nTrain Epoch: 29 [46080/60000 (77%)]\tLoss: 0.000002\nTrain Epoch: 29 [46400/60000 (77%)]\tLoss: 0.000002\nTrain Epoch: 29 [46720/60000 (78%)]\tLoss: 0.000002\nTrain Epoch: 29 [47040/60000 (78%)]\tLoss: 0.000000\nTrain Epoch: 29 [47360/60000 (79%)]\tLoss: 0.000000\nTrain Epoch: 29 [47680/60000 (79%)]\tLoss: 0.000011\nTrain Epoch: 29 [48000/60000 (80%)]\tLoss: 0.000000\nTrain Epoch: 29 [48320/60000 (81%)]\tLoss: 0.000000\nTrain Epoch: 29 [48640/60000 (81%)]\tLoss: 0.000000\nTrain Epoch: 29 [48960/60000 (82%)]\tLoss: 0.000001\nTrain Epoch: 29 [49280/60000 (82%)]\tLoss: 0.000000\nTrain Epoch: 29 [49600/60000 (83%)]\tLoss: 0.000014\nTrain Epoch: 29 [49920/60000 (83%)]\tLoss: 0.000061\nTrain Epoch: 29 [50240/60000 (84%)]\tLoss: 0.000007\nTrain Epoch: 29 [50560/60000 (84%)]\tLoss: 0.000247\nTrain Epoch: 29 [50880/60000 (85%)]\tLoss: 0.000000\nTrain Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000003\nTrain Epoch: 29 [51520/60000 (86%)]\tLoss: 0.000066\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 29 [51840/60000 (86%)]\tLoss: 0.000008\nTrain Epoch: 29 [52160/60000 (87%)]\tLoss: 0.000000\nTrain Epoch: 29 [52480/60000 (87%)]\tLoss: 0.000056\nTrain Epoch: 29 [52800/60000 (88%)]\tLoss: 0.000001\nTrain Epoch: 29 [53120/60000 (89%)]\tLoss: 0.000008\nTrain Epoch: 29 [53440/60000 (89%)]\tLoss: 0.000002\nTrain Epoch: 29 [53760/60000 (90%)]\tLoss: 0.000000\nTrain Epoch: 29 [54080/60000 (90%)]\tLoss: 0.000000\nTrain Epoch: 29 [54400/60000 (91%)]\tLoss: 0.000000\nTrain Epoch: 29 [54720/60000 (91%)]\tLoss: 0.000000\nTrain Epoch: 29 [55040/60000 (92%)]\tLoss: 0.000018\nTrain Epoch: 29 [55360/60000 (92%)]\tLoss: 0.000281\nTrain Epoch: 29 [55680/60000 (93%)]\tLoss: 0.000000\nTrain Epoch: 29 [56000/60000 (93%)]\tLoss: 0.000001\nTrain Epoch: 29 [56320/60000 (94%)]\tLoss: 0.000000\nTrain Epoch: 29 [56640/60000 (94%)]\tLoss: 0.000050\nTrain Epoch: 29 [56960/60000 (95%)]\tLoss: 0.000000\nTrain Epoch: 29 [57280/60000 (95%)]\tLoss: 0.000000\nTrain Epoch: 29 [57600/60000 (96%)]\tLoss: 0.000000\nTrain Epoch: 29 [57920/60000 (97%)]\tLoss: 0.000000\nTrain Epoch: 29 [58240/60000 (97%)]\tLoss: 0.000014\nTrain Epoch: 29 [58560/60000 (98%)]\tLoss: 0.000014\nTrain Epoch: 29 [58880/60000 (98%)]\tLoss: 0.000001\nTrain Epoch: 29 [59200/60000 (99%)]\tLoss: 0.000000\nTrain Epoch: 29 [59520/60000 (99%)]\tLoss: 0.000000\nTrain Epoch: 29 [59840/60000 (100%)]\tLoss: 0.000099\n\nTest set: Average loss: 0.0056, Accuracy: 9811/10000 (98%)\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Question 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nclass svd_ffmodel(nn.Module):\n    #This defines the structure of the NN.\n    def __init__(self , layer_list ,  activation='relu'):\n        super(svd_ffmodel, self).__init__()\n        self.img_size = 28*28\n        self.fc1 = layer_list[0]          # fully connected layer 1\n        self.fc2 = layer_list[1]          # fully connected layer 2 \n        self.fc3 = layer_list[2]          # fully connected layer 3\n        self.fc4 = layer_list[3]          # fully connected layer 4\n        self.fc5 = layer_list[4]          # fully connected layer 5\n        self.out_layer = layer_list[5]    # output layer\n        #select the activation function\n        if(activation=='relu'):\n            self.activation_fn = nn.ReLU()\n        if(activation=='logistic_sigmoid'):\n            self.activation_fn = nn.LogSigmoid()\n\n    def forward(self, x):\n        #flatten the input vector\n        x = x.view(-1, self.img_size)\n        \n        #Linear Layer 1 /Activation\n        u, s, v = torch.svd(self.fc1.weight)\n        x = x.mm(torch.mm(torch.mm(u[:,:20], torch.diag(s[:20])), v[:,:20].t()))\n        x = self.activation_fn( x ) \n        \n        #Linear Layer 2 /Activation\n        u, s, v = torch.svd(self.fc2.weight)\n        x = x.mm(torch.mm(torch.mm(u[:,:20], torch.diag(s[:20])), v[:,:20].t()))\n        x = self.activation_fn(x) \n        \n        #Linear Layer 3 /Activation\n        u, s, v = torch.svd(self.fc3.weight)\n        x = x.mm(torch.mm(torch.mm(u[:,:20], torch.diag(s[:20])), v[:,:20].t()))\n        x = self.activation_fn(x)\n        \n        #Linear Layer 4 /Activation\n        u, s, v = torch.svd(self.fc4.weight)\n        x = x.mm(torch.mm(torch.mm(u[:,:20], torch.diag(s[:20])), v[:,:20].t()))\n        x = self.activation_fn(x)\n        #Linear Layer 5 /Activation\n        u, s, v = torch.svd(self.fc5.weight)\n        x = x.mm(torch.mm(torch.mm(u[:,:20], torch.diag(s[:20])), v[:,:20].t()))\n        x = self.activation_fn(x)\n        \n        out = self.out_layer(x)\n        #Softmax gets probabilities. \n        return out","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_base = Net(activation='relu')\nmodel_base.apply(init_weights_kaiman)\nif cuda:\n    model_base.cuda()\n            \nbase_layers = list(model_base.children())","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd_model = SVD_Net( base_layers )","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = 1e-4\n\n# --------------------------> activation =relu |  init_method=kaiman\nif cuda:\n    svd_model.cuda()\noptimizer_adam = optim.Adam(svd_model.parameters() , lr=0.00001)\ncriterion = nn.CrossEntropyLoss()\n#Adam Training\n\nmodel5_ADAM_accuracy  = train(epochs , svd_model ,  optimizer_adam)","execution_count":41,"outputs":[{"output_type":"stream","text":"Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000017\nTrain Epoch: 29 [320/60000 (1%)]\tLoss: 0.000008\nTrain Epoch: 29 [640/60000 (1%)]\tLoss: 0.000212\nTrain Epoch: 29 [960/60000 (2%)]\tLoss: 0.000257\nTrain Epoch: 29 [1280/60000 (2%)]\tLoss: 0.000219\nTrain Epoch: 29 [1600/60000 (3%)]\tLoss: 0.000467\nTrain Epoch: 29 [1920/60000 (3%)]\tLoss: 0.000451\nTrain Epoch: 29 [2240/60000 (4%)]\tLoss: 0.000084\nTrain Epoch: 29 [2560/60000 (4%)]\tLoss: 0.000060\nTrain Epoch: 29 [2880/60000 (5%)]\tLoss: 0.000933\nTrain Epoch: 29 [3200/60000 (5%)]\tLoss: 0.000062\nTrain Epoch: 29 [3520/60000 (6%)]\tLoss: 0.000132\nTrain Epoch: 29 [3840/60000 (6%)]\tLoss: 0.000026\nTrain Epoch: 29 [4160/60000 (7%)]\tLoss: 0.000131\nTrain Epoch: 29 [4480/60000 (7%)]\tLoss: 0.000116\nTrain Epoch: 29 [4800/60000 (8%)]\tLoss: 0.000034\nTrain Epoch: 29 [5120/60000 (9%)]\tLoss: 0.000009\nTrain Epoch: 29 [5440/60000 (9%)]\tLoss: 0.000124\nTrain Epoch: 29 [5760/60000 (10%)]\tLoss: 0.000516\nTrain Epoch: 29 [6080/60000 (10%)]\tLoss: 0.000812\nTrain Epoch: 29 [6400/60000 (11%)]\tLoss: 0.000585\nTrain Epoch: 29 [6720/60000 (11%)]\tLoss: 0.002319\nTrain Epoch: 29 [7040/60000 (12%)]\tLoss: 0.000719\nTrain Epoch: 29 [7360/60000 (12%)]\tLoss: 0.001687\nTrain Epoch: 29 [7680/60000 (13%)]\tLoss: 0.000083\nTrain Epoch: 29 [8000/60000 (13%)]\tLoss: 0.000070\nTrain Epoch: 29 [8320/60000 (14%)]\tLoss: 0.001708\nTrain Epoch: 29 [8640/60000 (14%)]\tLoss: 0.000460\nTrain Epoch: 29 [8960/60000 (15%)]\tLoss: 0.000311\nTrain Epoch: 29 [9280/60000 (15%)]\tLoss: 0.000155\nTrain Epoch: 29 [9600/60000 (16%)]\tLoss: 0.000772\nTrain Epoch: 29 [9920/60000 (17%)]\tLoss: 0.000755\nTrain Epoch: 29 [10240/60000 (17%)]\tLoss: 0.000338\nTrain Epoch: 29 [10560/60000 (18%)]\tLoss: 0.000065\nTrain Epoch: 29 [10880/60000 (18%)]\tLoss: 0.000393\nTrain Epoch: 29 [11200/60000 (19%)]\tLoss: 0.000981\nTrain Epoch: 29 [11520/60000 (19%)]\tLoss: 0.000351\nTrain Epoch: 29 [11840/60000 (20%)]\tLoss: 0.000298\nTrain Epoch: 29 [12160/60000 (20%)]\tLoss: 0.001122\nTrain Epoch: 29 [12480/60000 (21%)]\tLoss: 0.000119\nTrain Epoch: 29 [12800/60000 (21%)]\tLoss: 0.000404\nTrain Epoch: 29 [13120/60000 (22%)]\tLoss: 0.000027\nTrain Epoch: 29 [13440/60000 (22%)]\tLoss: 0.000469\nTrain Epoch: 29 [13760/60000 (23%)]\tLoss: 0.001021\nTrain Epoch: 29 [14080/60000 (23%)]\tLoss: 0.000088\nTrain Epoch: 29 [14400/60000 (24%)]\tLoss: 0.000712\nTrain Epoch: 29 [14720/60000 (25%)]\tLoss: 0.000222\nTrain Epoch: 29 [15040/60000 (25%)]\tLoss: 0.000269\nTrain Epoch: 29 [15360/60000 (26%)]\tLoss: 0.000466\nTrain Epoch: 29 [15680/60000 (26%)]\tLoss: 0.000396\nTrain Epoch: 29 [16000/60000 (27%)]\tLoss: 0.000376\nTrain Epoch: 29 [16320/60000 (27%)]\tLoss: 0.000150\nTrain Epoch: 29 [16640/60000 (28%)]\tLoss: 0.000141\nTrain Epoch: 29 [16960/60000 (28%)]\tLoss: 0.001127\nTrain Epoch: 29 [17280/60000 (29%)]\tLoss: 0.000352\nTrain Epoch: 29 [17600/60000 (29%)]\tLoss: 0.000066\nTrain Epoch: 29 [17920/60000 (30%)]\tLoss: 0.000013\nTrain Epoch: 29 [18240/60000 (30%)]\tLoss: 0.000386\nTrain Epoch: 29 [18560/60000 (31%)]\tLoss: 0.000287\nTrain Epoch: 29 [18880/60000 (31%)]\tLoss: 0.000186\nTrain Epoch: 29 [19200/60000 (32%)]\tLoss: 0.000292\nTrain Epoch: 29 [19520/60000 (33%)]\tLoss: 0.000131\nTrain Epoch: 29 [19840/60000 (33%)]\tLoss: 0.000404\nTrain Epoch: 29 [20160/60000 (34%)]\tLoss: 0.000417\nTrain Epoch: 29 [20480/60000 (34%)]\tLoss: 0.000036\nTrain Epoch: 29 [20800/60000 (35%)]\tLoss: 0.000126\nTrain Epoch: 29 [21120/60000 (35%)]\tLoss: 0.000171\nTrain Epoch: 29 [21440/60000 (36%)]\tLoss: 0.000050\nTrain Epoch: 29 [21760/60000 (36%)]\tLoss: 0.000106\nTrain Epoch: 29 [22080/60000 (37%)]\tLoss: 0.006658\nTrain Epoch: 29 [22400/60000 (37%)]\tLoss: 0.000675\nTrain Epoch: 29 [22720/60000 (38%)]\tLoss: 0.001138\nTrain Epoch: 29 [23040/60000 (38%)]\tLoss: 0.001365\nTrain Epoch: 29 [23360/60000 (39%)]\tLoss: 0.000397\nTrain Epoch: 29 [23680/60000 (39%)]\tLoss: 0.000213\nTrain Epoch: 29 [24000/60000 (40%)]\tLoss: 0.001613\nTrain Epoch: 29 [24320/60000 (41%)]\tLoss: 0.000125\nTrain Epoch: 29 [24640/60000 (41%)]\tLoss: 0.000021\nTrain Epoch: 29 [24960/60000 (42%)]\tLoss: 0.000134\nTrain Epoch: 29 [25280/60000 (42%)]\tLoss: 0.000269\nTrain Epoch: 29 [25600/60000 (43%)]\tLoss: 0.000101\nTrain Epoch: 29 [25920/60000 (43%)]\tLoss: 0.000135\nTrain Epoch: 29 [26240/60000 (44%)]\tLoss: 0.000037\nTrain Epoch: 29 [26560/60000 (44%)]\tLoss: 0.001838\nTrain Epoch: 29 [26880/60000 (45%)]\tLoss: 0.000309\nTrain Epoch: 29 [27200/60000 (45%)]\tLoss: 0.003591\nTrain Epoch: 29 [27520/60000 (46%)]\tLoss: 0.000099\nTrain Epoch: 29 [27840/60000 (46%)]\tLoss: 0.000195\nTrain Epoch: 29 [28160/60000 (47%)]\tLoss: 0.001035\nTrain Epoch: 29 [28480/60000 (47%)]\tLoss: 0.000059\nTrain Epoch: 29 [28800/60000 (48%)]\tLoss: 0.000050\nTrain Epoch: 29 [29120/60000 (49%)]\tLoss: 0.000206\nTrain Epoch: 29 [29440/60000 (49%)]\tLoss: 0.001045\nTrain Epoch: 29 [29760/60000 (50%)]\tLoss: 0.001742\nTrain Epoch: 29 [30080/60000 (50%)]\tLoss: 0.000123\nTrain Epoch: 29 [30400/60000 (51%)]\tLoss: 0.000049\nTrain Epoch: 29 [30720/60000 (51%)]\tLoss: 0.000050\nTrain Epoch: 29 [31040/60000 (52%)]\tLoss: 0.000688\nTrain Epoch: 29 [31360/60000 (52%)]\tLoss: 0.000009\nTrain Epoch: 29 [31680/60000 (53%)]\tLoss: 0.000093\nTrain Epoch: 29 [32000/60000 (53%)]\tLoss: 0.000001\nTrain Epoch: 29 [32320/60000 (54%)]\tLoss: 0.000318\nTrain Epoch: 29 [32640/60000 (54%)]\tLoss: 0.004477\nTrain Epoch: 29 [32960/60000 (55%)]\tLoss: 0.002404\nTrain Epoch: 29 [33280/60000 (55%)]\tLoss: 0.000077\nTrain Epoch: 29 [33600/60000 (56%)]\tLoss: 0.000085\nTrain Epoch: 29 [33920/60000 (57%)]\tLoss: 0.001471\nTrain Epoch: 29 [34240/60000 (57%)]\tLoss: 0.000411\nTrain Epoch: 29 [34560/60000 (58%)]\tLoss: 0.000012\nTrain Epoch: 29 [34880/60000 (58%)]\tLoss: 0.000013\nTrain Epoch: 29 [35200/60000 (59%)]\tLoss: 0.000027\nTrain Epoch: 29 [35520/60000 (59%)]\tLoss: 0.000152\nTrain Epoch: 29 [35840/60000 (60%)]\tLoss: 0.003509\nTrain Epoch: 29 [36160/60000 (60%)]\tLoss: 0.000271\nTrain Epoch: 29 [36480/60000 (61%)]\tLoss: 0.000031\nTrain Epoch: 29 [36800/60000 (61%)]\tLoss: 0.000062\nTrain Epoch: 29 [37120/60000 (62%)]\tLoss: 0.000353\nTrain Epoch: 29 [37440/60000 (62%)]\tLoss: 0.000033\nTrain Epoch: 29 [37760/60000 (63%)]\tLoss: 0.000099\nTrain Epoch: 29 [38080/60000 (63%)]\tLoss: 0.000006\nTrain Epoch: 29 [38400/60000 (64%)]\tLoss: 0.000030\nTrain Epoch: 29 [38720/60000 (65%)]\tLoss: 0.000224\nTrain Epoch: 29 [39040/60000 (65%)]\tLoss: 0.000056\nTrain Epoch: 29 [39360/60000 (66%)]\tLoss: 0.000082\nTrain Epoch: 29 [39680/60000 (66%)]\tLoss: 0.000111\nTrain Epoch: 29 [40000/60000 (67%)]\tLoss: 0.000210\nTrain Epoch: 29 [40320/60000 (67%)]\tLoss: 0.000090\nTrain Epoch: 29 [40640/60000 (68%)]\tLoss: 0.005676\nTrain Epoch: 29 [40960/60000 (68%)]\tLoss: 0.002369\nTrain Epoch: 29 [41280/60000 (69%)]\tLoss: 0.000181\nTrain Epoch: 29 [41600/60000 (69%)]\tLoss: 0.000596\nTrain Epoch: 29 [41920/60000 (70%)]\tLoss: 0.000172\nTrain Epoch: 29 [42240/60000 (70%)]\tLoss: 0.000156\nTrain Epoch: 29 [42560/60000 (71%)]\tLoss: 0.000183\nTrain Epoch: 29 [42880/60000 (71%)]\tLoss: 0.000229\nTrain Epoch: 29 [43200/60000 (72%)]\tLoss: 0.001077\nTrain Epoch: 29 [43520/60000 (73%)]\tLoss: 0.000066\nTrain Epoch: 29 [43840/60000 (73%)]\tLoss: 0.000074\nTrain Epoch: 29 [44160/60000 (74%)]\tLoss: 0.000362\nTrain Epoch: 29 [44480/60000 (74%)]\tLoss: 0.000034\nTrain Epoch: 29 [44800/60000 (75%)]\tLoss: 0.000078\nTrain Epoch: 29 [45120/60000 (75%)]\tLoss: 0.000009\nTrain Epoch: 29 [45440/60000 (76%)]\tLoss: 0.000219\nTrain Epoch: 29 [45760/60000 (76%)]\tLoss: 0.001014\nTrain Epoch: 29 [46080/60000 (77%)]\tLoss: 0.000714\nTrain Epoch: 29 [46400/60000 (77%)]\tLoss: 0.000320\nTrain Epoch: 29 [46720/60000 (78%)]\tLoss: 0.000165\nTrain Epoch: 29 [47040/60000 (78%)]\tLoss: 0.000174\nTrain Epoch: 29 [47360/60000 (79%)]\tLoss: 0.000192\nTrain Epoch: 29 [47680/60000 (79%)]\tLoss: 0.000087\nTrain Epoch: 29 [48000/60000 (80%)]\tLoss: 0.000039\nTrain Epoch: 29 [48320/60000 (81%)]\tLoss: 0.000057\nTrain Epoch: 29 [48640/60000 (81%)]\tLoss: 0.000130\nTrain Epoch: 29 [48960/60000 (82%)]\tLoss: 0.000504\nTrain Epoch: 29 [49280/60000 (82%)]\tLoss: 0.000816\nTrain Epoch: 29 [49600/60000 (83%)]\tLoss: 0.000030\nTrain Epoch: 29 [49920/60000 (83%)]\tLoss: 0.000021\nTrain Epoch: 29 [50240/60000 (84%)]\tLoss: 0.000837\nTrain Epoch: 29 [50560/60000 (84%)]\tLoss: 0.000596\nTrain Epoch: 29 [50880/60000 (85%)]\tLoss: 0.000098\nTrain Epoch: 29 [51200/60000 (85%)]\tLoss: 0.000135\nTrain Epoch: 29 [51520/60000 (86%)]\tLoss: 0.000352\nTrain Epoch: 29 [51840/60000 (86%)]\tLoss: 0.000015\n","name":"stdout"},{"output_type":"stream","text":"Train Epoch: 29 [52160/60000 (87%)]\tLoss: 0.000447\nTrain Epoch: 29 [52480/60000 (87%)]\tLoss: 0.000018\nTrain Epoch: 29 [52800/60000 (88%)]\tLoss: 0.000081\nTrain Epoch: 29 [53120/60000 (89%)]\tLoss: 0.000152\nTrain Epoch: 29 [53440/60000 (89%)]\tLoss: 0.001476\nTrain Epoch: 29 [53760/60000 (90%)]\tLoss: 0.000468\nTrain Epoch: 29 [54080/60000 (90%)]\tLoss: 0.000393\nTrain Epoch: 29 [54400/60000 (91%)]\tLoss: 0.001284\nTrain Epoch: 29 [54720/60000 (91%)]\tLoss: 0.000048\nTrain Epoch: 29 [55040/60000 (92%)]\tLoss: 0.000197\nTrain Epoch: 29 [55360/60000 (92%)]\tLoss: 0.000184\nTrain Epoch: 29 [55680/60000 (93%)]\tLoss: 0.000196\nTrain Epoch: 29 [56000/60000 (93%)]\tLoss: 0.000490\nTrain Epoch: 29 [56320/60000 (94%)]\tLoss: 0.000117\nTrain Epoch: 29 [56640/60000 (94%)]\tLoss: 0.000062\nTrain Epoch: 29 [56960/60000 (95%)]\tLoss: 0.000200\nTrain Epoch: 29 [57280/60000 (95%)]\tLoss: 0.000084\nTrain Epoch: 29 [57600/60000 (96%)]\tLoss: 0.000007\nTrain Epoch: 29 [57920/60000 (97%)]\tLoss: 0.000355\nTrain Epoch: 29 [58240/60000 (97%)]\tLoss: 0.000188\nTrain Epoch: 29 [58560/60000 (98%)]\tLoss: 0.000258\nTrain Epoch: 29 [58880/60000 (98%)]\tLoss: 0.000062\nTrain Epoch: 29 [59200/60000 (99%)]\tLoss: 0.000129\nTrain Epoch: 29 [59520/60000 (99%)]\tLoss: 0.000183\nTrain Epoch: 29 [59840/60000 (100%)]\tLoss: 0.000153\n\nTest set: Average loss: 0.0025, Accuracy: 9829/10000 (98%)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}